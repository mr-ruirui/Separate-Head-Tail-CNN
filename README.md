# Distant Supervised Relation Extraction with Separate Head-Tail CNN
  
## Overview  
Distant supervised relation extraction is an efficient and effective strategy for automatically labeling large-scale training data. However, it inevitably suffers from wrong labeling problem. Most previous PCNN-based studies used piecewise pooling which may be insufficient to extract rich semantic features such as asymmetry. In this paper, we instead propose Separate Head-Tail CNN, a novel neural relation extraction framework to alleviate this issue. In this method, we obtain sentence representation by applying two non-weight-sharing groups of filters for separate convolution and pooling to the head and tail entity respectively. When classifying instances on bag level, we use coarse-to-fine strategy to filter out NA instances, leaving non-NA instances for further fine-grained relation classification. Experiments conducted on real-world datasets show that our model achieves significant and consistent improvements in relation extraction compared to baseline methods.

## Requirements
- Python  (>=3.6)
- Pytorch (==1.0)
- CUDA    (>=9.0)
- Matplotlib (>=3.0.2)

## Usage
### Dataset
We evaluate our model on a widely used dataset New York Times (NYT) released by (Riedel et al.,2010). The dataset was generated by aligning Freebase (Bollacker et al., 2008) relations with New York Times Corpus. Sentences of year 2005 and 2006 are used for training while sentences of 2007 are used as testing. There are 52 actual relations and a special NA which indicates there was no relation between two entities. The training set contains 522,611 sentences, 281,270 entity pairs and 18,152 relational facts. The testing set contains 172,448 sentences, 96,678 entity pairs and 1950 relational facts.

The origin data can be downloaded from [here](http://iesl.cs.umass.edu/riedel/ecml/). 


### Training
`python train.py --model_name shtcnn`

### Testing
`python test.py --model_name shtcnn`


